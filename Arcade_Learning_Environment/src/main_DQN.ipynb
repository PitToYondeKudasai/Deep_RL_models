{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from net import DQN\n",
    "from wrappers import make_env\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import typing as tt\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from buffer import Experience, ExperienceBuffer, BatchTensors\n",
    "import ale_py\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = make_env(config['env']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(config['model']['device'])\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "if load:\n",
    "    print('LOADING...')\n",
    "    data_loaded = torch.load('../models/Pong-v5' + \"-best_%.0f.dat\" % -9)\n",
    "\n",
    "    net.load_state_dict(data_loaded)\n",
    "    tgt_net.load_state_dict(data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ExperienceBuffer(config['replay_buffer']['size'])\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = config['agent']['epsilon_start']\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=float(config['agent']['learning_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_m_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
    "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
    "    for e in batch:\n",
    "        states.append(e.state)\n",
    "        actions.append(e.action)\n",
    "        rewards.append(e.reward)\n",
    "        dones.append(e.done_trunc)\n",
    "        new_state.append(e.new_state)\n",
    "    states_t = torch.as_tensor(np.asarray(states))\n",
    "    actions_t = torch.LongTensor(actions)\n",
    "    rewards_t = torch.FloatTensor(rewards)\n",
    "    dones_t = torch.BoolTensor(dones)\n",
    "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
    "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
    "           dones_t.to(device),  new_states_t.to(device)\n",
    "\n",
    "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
    "              device: torch.device) -> torch.Tensor:\n",
    "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
    "\n",
    "    state_action_values = net(states_t).gather(\n",
    "        1, actions_t.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(new_states_t).max(1)[0]\n",
    "        next_state_values[dones_t] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * config['agent']['gamma']+ rewards_t\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(config['agent']['epsilon_final'], config['agent']['epsilon_start'] - frame_idx / config['agent']['epsilon_decay_last_frame'])\n",
    "    reward = agent.play_step(net, device, epsilon)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        m_reward = np.mean(total_rewards[-100:])\n",
    "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
    "        if best_m_reward is None or best_m_reward < m_reward:\n",
    "            torch.save(net.state_dict(), '../models/Pong-v5' + \"-best_%.0f.dat\" % m_reward)\n",
    "            if best_m_reward is not None:\n",
    "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
    "            best_m_reward = m_reward\n",
    "        if m_reward > config['agent']['mean_rew_bound']:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "    if len(buffer) < config['replay_buffer']['replay_start_size']:\n",
    "        continue\n",
    "    if frame_idx % config['agent']['sync_target_frames']== 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(config['agent']['batch_size'])\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
